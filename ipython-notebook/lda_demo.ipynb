{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "from gensim.models import hdpmodel, ldamodel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from itertools import izip\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "computer_scientists = [\"Alan Turing\", \"Donal Knuth\", \"Edsger W. Dijkstra\", \"John von Neumann\", \"Grace Hopper\", \n",
    "                       \"Shafi Goldwasser\"]\n",
    "\n",
    "fruits = [\"Banana\", \"Apple\", \"Oranges (fruit)\", \"Citron\", \"Mango\", \"Pineapple\"]\n",
    "\n",
    "\n",
    "documents = [wikipedia.page(elem).content for elem in (fruits + computer_scientists)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocessing(documents):\n",
    "    # remove common words and tokenize\n",
    "    special_chars = set('== === von turing neumann knuth apple banana mango citron hopper dijkstra'.split())\n",
    "    texts = [[word for word in document.lower().split() if word not in STOPWORDS and word not in special_chars] \n",
    "             for document in documents]\n",
    "\n",
    "    # remove words that appear only once\n",
    "    all_tokens = sum(texts, [])\n",
    "    tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)\n",
    "    texts = [[word for word in text if word not in tokens_once] for text in texts]\n",
    "\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    return corpus, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus, dictionary = preprocessing(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.99979323332526382)]\n",
      "TOPIC #0:   0.006*university + 0.006*programming + 0.005*work + 0.005*mathematical + 0.004*science + 0.004*computing + 0.004*edsger + 0.004*new + 0.003*w. + 0.003*neumann's\n",
      "\n",
      "\n",
      "TOPIC #1:   0.010*fruit + 0.007*orange + 0.007*grown + 0.006*bananas + 0.005*pineapple + 0.004*apples + 0.004*cultivars + 0.004*citrus + 0.004*called + 0.004*sweet\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TOPICS = 2\n",
    "lda = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=TOPICS, update_every=1, chunksize=1, passes=100)\n",
    "corpus_lda = lda[corpus]\n",
    "\n",
    "print lda[corpus[0]]    # get topic probability distribution for a document\n",
    "\n",
    "#lda.print_topics()\n",
    "for i in range(TOPICS):\n",
    "    print \"TOPIC #\" + str(i) + \":   \"+ lda.print_topic(i)\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
